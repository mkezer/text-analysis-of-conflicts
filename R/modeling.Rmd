---
title: "Modeling"
output: 
    html_document:
      theme: cerulean
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 3
      code_folding: hide
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      tidy = TRUE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
```

```{r}
library(here)
library(rio)

library(tidyverse)
library(magrittr)

library(recipes)
library(caret)
library(ranger)
library(MLmetrics)
library(vip)

library(knitr)
library(kableExtra)

library(doParallel)
```

```{r}
metrics <- function(observed, predicted, model){
  
  MAE <- mean(abs(observed - predicted))
  RMSE <- sqrt(mean((observed - predicted)^2))
  R2 <- cor(observed, predicted)^2
  
  table <-
  tibble::tibble(
    Model = model,
    MAE = MAE,
    RMSE = RMSE,
    R_squared = R2
  )
  
  return(table)
}
```


## Data
```{r cache=TRUE}
# data files
df <- import(here("data", "df_final.csv"))

df %<>% 
  select(participant, p_right, everything(),
         -conflict_text, -study) %>% 
  mutate(across(participant, as.factor)) %>% 
  janitor::clean_names()
```

## Outcome 1: Judgment of self {.tabset .tabset-fade}
### Prepare the recipe
```{r}
set.seed(45)

# Train/Test Split
loc      <- sample(1:nrow(df), round(nrow(df) * 0.8))
df_tr  <- df[loc, ]
df_te  <- df[-loc, ]

# Cross validation settings
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds <- cut(seq(1, nrow(df_tr)),
             breaks = 10,
             labels = FALSE)
  
# Create the list for each fold 
my.indices <- vector('list',10)

for(i in 1:10){
  my.indices[[i]] <- which(folds != i)
}

cv <- trainControl(method = "cv",
                   index  = my.indices)

# Blueprint
blueprint <- recipe(x     = df,
                    vars  = colnames(df),
                    roles = c("id", "outcome", rep("predictor", 1095))
                    ) %>% 
  step_zv(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_impute_mean(all_numeric()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), 
            threshold = 0.8) %>% 
  step_rm("t_right", "condition")

blueprint
```

### Linear regression with ridge penalty
```{r, cache = TRUE}
# Tune Grid
grid <- data.frame(alpha = 0, 
                   lambda = seq(0.01, 3, 0.01))
  
# Train the model
ridge <- caret::train(blueprint, 
                      data      = df_tr, 
                      method    = "glmnet", 
                      trControl = cv,
                      tuneGrid  = grid)
  
plot(ridge)
ridge$bestTune  

# Tune grid v2
grid2 <- data.frame(alpha = 0, 
                    lambda = seq(3, 20, 1))
  
# Train the model v2
ridge2 <- caret::train(blueprint, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv,
                       tuneGrid  = grid2)

plot(ridge2)
ridge2$bestTune

# Tune grid v3
grid3 <- data.frame(alpha = 0, 
                    lambda = seq(6, 8, .01))
  
# Train the model v3
ridge3 <- caret::train(blueprint, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv,
                       tuneGrid  = grid3)
  
plot(ridge3)
ridge3$bestTune

predict_te_ridge3 <- predict(ridge3, df_te)

vip(ridge3, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()

ridge_eval <-
  tibble(
    Model = "Ridge",
    RMSE = RMSE(predict_te_ridge3, df_te$p_right),
    MAE = MAE(predict_te_ridge3, df_te$p_right),
    R2 = R2_Score(predict_te_ridge3, df_te$p_right)
  )

kable(ridge_eval,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

### Linear regression with lasso penalty
```{r, cache = TRUE}
# Cross validation settings
set.seed(46)
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds2 <- cut(seq(1, nrow(df_tr)),
             breaks = 10,
             labels = FALSE)
  
# Create the list for each fold 
my.indices2 <- vector('list',10)

for(i in 1:10){
  my.indices2[[i]] <- which(folds2 != i)
}

cv2 <- trainControl(method = "cv",
                    index  = my.indices2)
  
# Tune Grid  
grid4 <- data.frame(alpha = 1, 
                    lambda = seq(0.01, 3, 0.01)) 
    
# Train the model
lasso <- caret::train(blueprint, 
                      data      = df_tr, 
                      method    = "glmnet", 
                      trControl = cv2,
                      tuneGrid  = grid4)

plot(lasso)
lasso$bestTune

predict_te_lasso <- predict(lasso, df_te)

vip(lasso, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()

lasso_eval <-
  tibble(
    Model = "Lasso",
    RMSE = RMSE(predict_te_lasso, df_te$p_right),
    MAE = MAE(predict_te_lasso, df_te$p_right),
    R2 = R2_Score(predict_te_lasso, df_te$p_right)
  )

kable(lasso_eval,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

### Gradient Boosting Trees
```{r}
# Cross validation settings 
set.seed(47)
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds3 <- cut(seq(1,nrow(df_tr)),
              breaks = 10,
              labels = FALSE)

# Create the list containing the row indices for each fold 
my.indices3 <- vector('list', 10)

for(i in 1:10){
  my.indices3[[i]] <- which(folds3 != i)
}

cv3 <- trainControl(method = "cv",
                    index  = my.indices3)

# Set the multiple cores for parallel processing
ncores <- 4   
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)

## Step 1: Tune the number of trees
grid5 <- expand.grid(shrinkage         = 0.1,
                     n.trees           = 1:1000,
                     interaction.depth = 5,
                     n.minobsinnode    = 10)
 
gbm1 <- caret::train(blueprint,
                     data         = df_tr,
                     method       = "gbm",
                     trControl    = cv3,
                     tuneGrid     = grid5,
                     bag.fraction = 1,
                     verbose      = FALSE)

plot(gbm1, type = "l")

## Step 2: Tune the interaction depth and minimum number of observations
grid6 <- expand.grid(shrinkage         = 0.1,
                     n.trees           = 200,
                     interaction.depth = 1:15,
                     n.minobsinnode    = c(5,10,20,30,40,50))


gbm2 <- caret::train(blueprint,
                     data      = df_tr,
                     method    = "gbm",
                     trControl = cv3,
                     tuneGrid  = grid6,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm2, type = "l")

## Step 3: Lower the learning rate and increase the number of trees
grid7 <- expand.grid(shrinkage         = 0.01,
                     n.trees           = 1:5000,
                     interaction.depth = 9,
                     n.minobsinnode    = 40)


gbm3 <- caret::train(blueprint_readability,
                     data      = read_tr,
                     method    = 'gbm',
                     trControl = cv3,
                     tuneGrid  = grid7,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm3, type = "l")
```


## Outcome 2: Judgment of the other person {.tabset .tabset-fade}
### Prepare the recipe
```{r}
set.seed(50)

df %<>% 
  relocate(t_right, .after = participant)

# Train/Test Split
loc      <- sample(1:nrow(df), round(nrow(df) * 0.8))
df_tr  <- df[loc, ]
df_te  <- df[-loc, ]

# Cross validation settings
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)), ]

# Create 10 folds with equal size
folds4 <- cut(seq(1, nrow(df_tr)),
              breaks = 10,
              labels = FALSE)
  
# Create the list for each fold 
my.indices4 <- vector('list',10)

for(i in 1:10){
  my.indices4[[i]] <- which(folds4 != i)
}

cv4 <- trainControl(method = "cv",
                    index  = my.indices4)

# Blueprint
blueprint2 <- recipe(x     = df,
                     vars  = colnames(df),
                     roles = c("id", "outcome", rep("predictor", 1095))
                    ) %>% 
  step_zv(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_impute_mean(all_numeric()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), 
            threshold = 0.8) %>% 
  step_rm("p_right", "condition")

blueprint2
```

### Linear regression with ridge penalty
```{r}
# Tune Grid
grid8 <- data.frame(alpha = 0, 
                    lambda = seq(0.01, 3, 0.01))
  
# Train the model
ridge4 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv4,
                       tuneGrid  = grid8)
  
plot(ridge4)
ridge4$bestTune  

# Tune grid v2
grid9 <- data.frame(alpha = 0, 
                    lambda = seq(3, 20, 1))
  
# Train the model v2
ridge5 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv4,
                       tuneGrid  = grid9)

plot(ridge5)
ridge5$bestTune

# Tune grid v3
grid10 <- data.frame(alpha = 0, 
                     lambda = seq(3.5, 4.5, .01))
  
# Train the model v3
ridge6 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv4,
                       tuneGrid  = grid10)
  
plot(ridge6)
ridge6$bestTune

predict_te_ridge4 <- predict(ridge6, df_te)

vip(ridge6, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()

ridge_eval2 <- metrics(df_te$p_right, predict_te_ridge4, "Ridge")

kable(ridge_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

### Linear regression with lasso penalty
```{r}
# Cross validation settings
set.seed(51)
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds5 <- cut(seq(1, nrow(df_tr)),
              breaks = 10,
              labels = FALSE)
  
# Create the list for each fold 
my.indices5 <- vector('list',10)

for(i in 1:10){
  my.indices5[[i]] <- which(folds5 != i)
}

cv5 <- trainControl(method = "cv",
                    index  = my.indices5)
  
# Tune Grid  
# Note that we set the value of alpha to 1 for lasso regression
grid11 <- data.frame(alpha = 1, 
                    lambda = seq(0.01, 3, 0.01)) 
    
# Train the model
lasso2 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv5,
                       tuneGrid  = grid11)

plot(lasso2)
lasso2$bestTune

predict_te_lasso2 <- predict(lasso2, df_te)

vip(lasso2, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()

lasso_eval2 <-
  tibble(
    Model = "Lasso",
    RMSE = RMSE(predict_te_lasso2, df_te$p_right),
    MAE = MAE(predict_te_lasso2, df_te$p_right),
    R2 = R2_Score(predict_te_lasso2, df_te$p_right)
  )

kable(lasso_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

### Gradient Boosting Trees
```{r}
# Cross validation settings 
set.seed(52)
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds6 <- cut(seq(1,nrow(df_tr)),
              breaks = 10,
              labels = FALSE)

# Create the list containing the row indices for each fold 
my.indices6 <- vector('list', 10)

for(i in 1:10){
  my.indices6[[i]] <- which(folds6 != i)
}

cv6 <- trainControl(method = "cv",
                    index  = my.indices6)

# Set the multiple cores for parallel processing
ncores <- 4   
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)

## Step 1: Tune the number of trees
grid12 <- expand.grid(shrinkage         = 0.1,
                      n.trees           = 1:1000,
                      interaction.depth = 5,
                      n.minobsinnode    = 10)

gbm4 <- caret::train(blueprint2,
                     data         = df_tr,
                     method       = "gbm",
                     trControl    = cv6,
                     tuneGrid     = grid12,
                     bag.fraction = 1,
                     verbose      = FALSE)

plot(gbm4, type = "l")

## Step 2: Tune the interaction depth and minimum number of observations
grid13 <- expand.grid(shrinkage         = 0.1,
                      n.trees           = 200,
                      interaction.depth = 1:15,
                      n.minobsinnode    = c(5,10,20,30,40,50))


gbm5 <- caret::train(blueprint2,
                     data      = df_tr,
                     method    = "gbm",
                     trControl = cv6,
                     tuneGrid  = grid12,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm5, type = "l")

## Step 3: Lower the learning rate and increase the number of trees
grid14 <- expand.grid(shrinkage         = 0.01,
                      n.trees           = 1:5000,
                      interaction.depth = 9,
                      n.minobsinnode    = 40)


gbm6 <- caret::train(blueprint2,
                     data      = df_tr,
                     method    = 'gbm',
                     trControl = cv6,
                     tuneGrid  = grid14,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm6, type = "l")
```

## Summary of results
```{r}

```

```{r}
save.image(here("data", "modeling.RData"))

```




