---
title: "Modeling"
output: 
    html_document:
      theme: cerulean
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 3
      code_folding: hide
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      tidy = TRUE, 
                      cache = FALSE, 
                      warning = FALSE, 
                      message = FALSE,
                      eval = FALSE)
```

```{r}
library(here)
library(rio)


library(magrittr)

library(recipes)
library(caret)
library(ranger)
library(MLmetrics)
library(vip)

library(doParallel)
```

```{r, eval=TRUE}
metrics <- function(observed, predicted, model){
  
  MAE <- mean(abs(observed - predicted))
  RMSE <- sqrt(mean((observed - predicted)^2))
  R2 <- cor(observed, predicted)^2
  
  table <-
  tibble::tibble(
    Model = model,
    MAE = MAE,
    RMSE = RMSE,
    R_squared = R2
  )
  
  return(table)
}

load(here::here("R", "modeling_gbm6.RData"))

library(knitr)
library(kableExtra)
library(tidyverse)
```


## Data
```{r}
# data files
df <- import(here("data", "df_final.csv"))

df %<>% 
  select(participant, p_right, everything(),
         -conflict_text, -study) %>% 
  mutate(across(participant, as.factor)) %>% 
  janitor::clean_names()
```

```{r, eval=TRUE}
head(df[1:10])
```

## Outcome 1: Judgment of self {.tabset .tabset-fade}
### Prepare the recipe
```{r}
set.seed(45)

# Train/Test Split
loc      <- sample(1:nrow(df), round(nrow(df) * 0.8))
df_tr  <- df[loc, ]
df_te  <- df[-loc, ]

# Cross validation settings
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds <- cut(seq(1, nrow(df_tr)),
             breaks = 10,
             labels = FALSE)
  
# Create the list for each fold 
my.indices <- vector('list',10)

for(i in 1:10){
  my.indices[[i]] <- which(folds != i)
}

cv <- trainControl(method = "cv",
                   index  = my.indices)

# Blueprint
blueprint <- recipe(x     = df,
                    vars  = colnames(df),
                    roles = c("id", "outcome", rep("predictor", 1095))
                    ) %>% 
  step_zv(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_impute_mean(all_numeric()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), 
            threshold = 0.8) %>% 
  step_rm("t_right", "condition")
```

```{r, eval=TRUE}
blueprint
```


### Linear regression with ridge penalty
```{r}
# Tune Grid
grid <- data.frame(alpha = 0, 
                   lambda = seq(0.01, 3, 0.01))
  
# Train the model
ridge <- caret::train(blueprint, 
                      data      = df_tr, 
                      method    = "glmnet", 
                      trControl = cv,
                      tuneGrid  = grid)
  
plot(ridge)
ridge$bestTune  

# Tune grid v2
grid2 <- data.frame(alpha = 0, 
                    lambda = seq(3, 20, 1))
  
# Train the model v2
ridge2 <- caret::train(blueprint, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv,
                       tuneGrid  = grid2)

plot(ridge2)
ridge2$bestTune

# Tune grid v3
grid3 <- data.frame(alpha = 0, 
                    lambda = seq(6, 8, .01))
  
# Train the model v3
ridge3 <- caret::train(blueprint, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv,
                       tuneGrid  = grid3)
  
plot(ridge3)
ridge3$bestTune

predict_te_ridge3 <- predict(ridge3, df_te)

vip(ridge3, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()

ridge_eval <- metrics(df_te$p_right, predict_te_ridge3, "Ridge")
```

```{r, eval=TRUE}
plot(ridge3)
ridge3$bestTune

kable(ridge_eval,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

### Linear regression with lasso penalty
```{r}
# Cross validation settings
set.seed(46)
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds2 <- cut(seq(1, nrow(df_tr)),
             breaks = 10,
             labels = FALSE)
  
# Create the list for each fold 
my.indices2 <- vector('list',10)

for(i in 1:10){
  my.indices2[[i]] <- which(folds2 != i)
}

cv2 <- trainControl(method = "cv",
                    index  = my.indices2)
  
# Tune Grid  
grid4 <- data.frame(alpha = 1, 
                    lambda = seq(0.01, 3, 0.01)) 
    
# Train the model
lasso <- caret::train(blueprint, 
                      data      = df_tr, 
                      method    = "glmnet", 
                      trControl = cv2,
                      tuneGrid  = grid4)

plot(lasso)
lasso$bestTune

predict_te_lasso <- predict(lasso, df_te)

vip(lasso, 
    num_features = 10, 
    geom = "point") + 
  theme_bw()

lasso_eval <- metrics(df_te$p_right, predict_te_lasso, "Lasso")

kable(lasso_eval,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

```{r, eval=TRUE}
plot(lasso)
lasso$bestTune

kable(lasso_eval,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```


### Gradient Boosting Trees
```{r}
# Cross validation settings 
set.seed(47)
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds3 <- cut(seq(1,nrow(df_tr)),
              breaks = 10,
              labels = FALSE)

# Create the list containing the row indices for each fold 
my.indices3 <- vector('list', 10)

for(i in 1:10){
  my.indices3[[i]] <- which(folds3 != i)
}

cv3 <- trainControl(method = "cv",
                    index  = my.indices3)

# Set the multiple cores for parallel processing
ncores <- 4   
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)

## Step 1: Tune the number of trees
grid5 <- expand.grid(shrinkage         = 0.1,
                     n.trees           = 1:1000,
                     interaction.depth = 5,
                     n.minobsinnode    = 10)
 
gbm1 <- caret::train(blueprint,
                     data         = df_tr,
                     method       = "gbm",
                     trControl    = cv3,
                     tuneGrid     = grid5,
                     bag.fraction = 1,
                     verbose      = FALSE)

plot(gbm1, type = "l")
gbm1$bestTune

## Step 2: Tune the interaction depth and minimum number of observations
grid6 <- expand.grid(shrinkage         = 0.1,
                     n.trees           = 200,
                     interaction.depth = 1:15,
                     n.minobsinnode    = c(5,10,20,30,40,50))


gbm2 <- caret::train(blueprint,
                     data      = df_tr,
                     method    = "gbm",
                     trControl = cv3,
                     tuneGrid  = grid6,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm2, type = "l")

## Step 3: Lower the learning rate and increase the number of trees
grid7 <- expand.grid(shrinkage         = 0.01,
                     n.trees           = 1:5000,
                     interaction.depth = 9,
                     n.minobsinnode    = 40)


gbm3 <- caret::train(blueprint_readability,
                     data      = read_tr,
                     method    = 'gbm',
                     trControl = cv3,
                     tuneGrid  = grid7,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm3, type = "l")
gbm3$bestTune

predict_te_gbm1 <- predict(gbm3, df_te)

gmb_eval <- metrics(df_te$p_right, predict_te_gbm1, 
                    "Gradient Boosting Trees")

kable(gmb_eval,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")

vip_gbm <-
vip(gbm3,
    num_features = 10,
    geom = "point") +
  theme_bw() 

df_te %<>% 
  mutate(gbm_self = predict_te_gbm1)

gbm_self <-
df_te %>% 
  mutate(participant = 1:nrow(df_te)) %>% 
  select(participant, p_right, gbm_self) %>% 
  gather(var, val, -participant) %>% 
  mutate(var = recode(var,
                      `gbm_self` = "Predicted",
                      `t_right` = "Observed")
         ) %>%   
  ggplot() +
  geom_point(aes(participant, val, color = var)) +
  theme_bw() +
    theme(legend.position = "none") +
  labs(y = "",
       x = "Participants",
       title = "Judgment of the self",
       color = "") +
  scale_y_continuous(breaks = 1:7)
```

```{r, eval=TRUE}
plot(gbm3, type = "l")
gbm3$bestTune

kable(gmb_eval,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

## Outcome 2: Judgment of the other person {.tabset .tabset-fade}
### Prepare the recipe
```{r}
set.seed(50)

df %<>% 
  relocate(t_right, .after = participant)

# Train/Test Split
loc      <- sample(1:nrow(df), round(nrow(df) * 0.8))
df_tr  <- df[loc, ]
df_te  <- df[-loc, ]

# Cross validation settings
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)), ]

# Create 10 folds with equal size
folds4 <- cut(seq(1, nrow(df_tr)),
              breaks = 10,
              labels = FALSE)
  
# Create the list for each fold 
my.indices4 <- vector('list',10)

for(i in 1:10){
  my.indices4[[i]] <- which(folds4 != i)
}

cv4 <- trainControl(method = "cv",
                    index  = my.indices4)

# Blueprint
blueprint2 <- recipe(x     = df,
                     vars  = colnames(df),
                     roles = c("id", "outcome", rep("predictor", 1095))
                    ) %>% 
  step_zv(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_impute_mean(all_numeric()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors(), 
            threshold = 0.8) %>% 
  step_rm("p_right", "condition")


```

```{r, eval=TRUE}
blueprint2
```


### Linear regression with ridge penalty
```{r}
# Tune Grid
grid8 <- data.frame(alpha = 0, 
                    lambda = seq(0.01, 3, 0.01))
  
# Train the model
ridge4 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv4,
                       tuneGrid  = grid8)
  
plot(ridge4)
ridge4$bestTune  

# Tune grid v2
grid9 <- data.frame(alpha = 0, 
                    lambda = seq(3, 20, 1))
  
# Train the model v2
ridge5 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv4,
                       tuneGrid  = grid9)

plot(ridge5)
ridge5$bestTune

# Tune grid v3
grid10 <- data.frame(alpha = 0, 
                     lambda = seq(3.5, 4.5, .01))
  
# Train the model v3
ridge6 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv4,
                       tuneGrid  = grid10)
  
plot(ridge6)
ridge6$bestTune

predict_te_ridge4 <- predict(ridge6, df_te)

ridge_eval2 <- metrics(df_te$p_right, predict_te_ridge4, "Ridge")

kable(ridge_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")

vip_ridge <-
vip(ridge6,
    num_features = 10,
    geom = "point") +
  theme_bw() 

df_te %<>% 
  mutate(ridge_other = predict_te_ridge4)

ridge_p <-
df_te %>% 
  mutate(participant = 1:nrow(df_te)) %>% 
  select(participant, t_right, ridge_other) %>% 
  gather(var, val, -participant) %>% 
  mutate(var = recode(var,
                      `ridge_other` = "Predicted",
                      `t_right` = "Observed")
         ) %>% 
  ggplot() +
  geom_point(aes(participant, val, color = var)) +
  theme_bw() + 
  theme(legend.position = "none") +
  labs(y = "",
       x = "Participants",
       title = "Judgment of the other",
       color = "") +
  scale_y_continuous(breaks = 1:7)
  
```

```{r, eval=TRUE}
plot(ridge6)
ridge6$bestTune

kable(ridge_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```


### Linear regression with lasso penalty
```{r}
# Cross validation settings
set.seed(51)
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds5 <- cut(seq(1, nrow(df_tr)),
              breaks = 10,
              labels = FALSE)
  
# Create the list for each fold 
my.indices5 <- vector('list',10)

for(i in 1:10){
  my.indices5[[i]] <- which(folds5 != i)
}

cv5 <- trainControl(method = "cv",
                    index  = my.indices5)
  
# Tune Grid  
# Note that we set the value of alpha to 1 for lasso regression
grid11 <- data.frame(alpha = 1, 
                    lambda = seq(0.01, 3, 0.01)) 
    
# Train the model
lasso2 <- caret::train(blueprint2, 
                       data      = df_tr, 
                       method    = "glmnet", 
                       trControl = cv5,
                       tuneGrid  = grid11)

plot(lasso2)
lasso2$bestTune

predict_te_lasso2 <- predict(lasso2, df_te)

lasso_eval2 <- metrics(df_te$p_right, predict_te_lasso2, "Lasso")

kable(lasso_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

```{r, eval=TRUE}
plot(lasso2)
lasso2$bestTune

kable(lasso_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```


### Gradient Boosting Trees
```{r}
# Cross validation settings 
set.seed(52)
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds6 <- cut(seq(1,nrow(df_tr)),
              breaks = 10,
              labels = FALSE)

# Create the list containing the row indices for each fold 
my.indices6 <- vector('list', 10)

for(i in 1:10){
  my.indices6[[i]] <- which(folds6 != i)
}

cv6 <- trainControl(method = "cv",
                    index  = my.indices6)

# Set the multiple cores for parallel processing
ncores <- 4   
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)

## Step 1: Tune the number of trees
grid12 <- expand.grid(shrinkage         = 0.1,
                      n.trees           = 1:1000,
                      interaction.depth = 5,
                      n.minobsinnode    = 10)

gbm4 <- caret::train(blueprint2,
                     data         = df_tr,
                     method       = "gbm",
                     trControl    = cv6,
                     tuneGrid     = grid12,
                     bag.fraction = 1,
                     verbose      = FALSE)

plot(gbm4, type = "l")
gbm4$bestTune

## Step 2: Tune the interaction depth and minimum number of observations
grid13 <- expand.grid(shrinkage         = 0.1,
                      n.trees           = 200,
                      interaction.depth = 1:15,
                      n.minobsinnode    = c(5,10,20,30,40,50))


gbm5 <- caret::train(blueprint2,
                     data      = df_tr,
                     method    = "gbm",
                     trControl = cv6,
                     tuneGrid  = grid12,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm5, type = "l")
gbm5$bestTune

## Step 3: Lower the learning rate and increase the number of trees
grid14 <- expand.grid(shrinkage         = 0.01,
                      n.trees           = 1:5000,
                      interaction.depth = 9,
                      n.minobsinnode    = 40)


gbm6 <- caret::train(blueprint2,
                     data      = df_tr,
                     method    = 'gbm',
                     trControl = cv6,
                     tuneGrid  = grid14,
                     bag.fraction = 1,
                     verbose = FALSE)

plot(gbm6, type = "l")
gbm6$bestTune

predict_te_gbm2 <- predict(gbm6, df_te)

gmb_eval2 <- metrics(df_te$p_right, predict_te_gbm2, 
                    "Gradient Boosting Trees")

kable(gmb_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```

```{r, eval=TRUE}
plot(gbm6, type = "l")
gbm6$bestTune

kable(gmb_eval2,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white")
```


## Summary of results {.tabset .tabset-fade}
### Table
```{r, eval=TRUE}
eval_table <-
  bind_rows(ridge_eval, lasso_eval, gmb_eval,
            ridge_eval2, lasso_eval2, gmb_eval2)

kable(eval_table,
      digits = 4) %>% 
  kable_styling(bootstrap_options = c("striped"),
                full_width = FALSE) %>% 
  row_spec(0, background = "gray", color = "white") %>% 
  pack_rows("Judgment of self", 1, 3) %>% 
  pack_rows("Judgment of other", 4, 6)
```

### Plots
```{r,eval=TRUE}
library(ggpubr)
library(vip)
library(gbm)
library(magrittr)

vip_gbm <-
vip(gbm3,
    num_features = 10,
    geom = "point") +
  theme_bw() 

vip_ridge <-
vip(ridge6,
    num_features = 10,
    geom = "point") +
  theme_bw()

p1 <-
ggarrange(vip_gbm, vip_ridge, 
          ncol = 2, 
          nrow = 1,
          labels = c("A", "B"))

p1

# ggsave(filename = "fet_imp.png",
#        plot = p1,
#        device = "png",
#        path = here("report"),
#        units = "cm",
#        width = 20,
#        height = 15)

df_te %<>% 
  mutate(gbm_self = predict_te_gbm1)

gbm_self <-
df_te %>% 
  mutate(participant = 1:nrow(df_te)) %>% 
  select(participant, p_right, gbm_self) %>% 
  gather(var, val, -participant) %>% 
  mutate(var = recode(var,
                      `gbm_self` = "Predicted",
                      `t_right` = "Observed")
         ) %>%   
  ggplot() +
  geom_point(aes(participant, val, color = var)) +
  theme_bw() +
    theme(legend.position = "none") +
  labs(y = "",
       x = "Participants",
       title = "Judgment of the self",
       color = "") +
  scale_y_continuous(breaks = 1:7)


df_te %<>% 
  mutate(ridge_other = predict_te_ridge4)

ridge_p <-
df_te %>% 
  mutate(participant = 1:nrow(df_te)) %>% 
  select(participant, t_right, ridge_other) %>% 
  gather(var, val, -participant) %>% 
  mutate(var = recode(var,
                      `ridge_other` = "Predicted",
                      `t_right` = "Observed")
         ) %>% 
  ggplot() +
  geom_point(aes(participant, val, color = var)) +
  theme_bw() + 
  theme(legend.position = "none") +
  labs(y = "",
       x = "Participants",
       title = "Judgment of the other",
       color = "") +
  scale_y_continuous(breaks = 1:7)

p2 <-
ggarrange(gbm_self, ridge_p,
          ncol = 2,
          nrow = 1, 
          common.legend = TRUE, 
          legend = "bottom")

p2

# ggsave(filename = "obs_pred.png",
#        plot = p2,
#        device = "png",
#        path = here("report"),
#        units = "cm",
#        width = 20,
#        height = 15)
```



