---
title: "Modeling"
output: 
    html_document:
      theme: cerulean
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 3
      code_folding: hide
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      tidy = TRUE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
```

```{r}
library(here)
library(rio)

library(tidyverse)
library(magrittr)

library(recipes)
library(caret)
library(ranger)
library(MLmetrics)
library(vip)

library(doParallel)
```

## Data
```{r cache=TRUE}
# data files
df <- import(here("data", "df_clean.csv"))

```

# Outcome 1: Judgment of self
## Prepare the recipe
```{r}
# Train/Test Split
loc      <- sample(1:nrow(df), round(nrow(df) * 0.8))
df_tr  <- df[loc, ]
df_te  <- df[-loc, ]

# Cross validation settings
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds <- cut(seq(1, nrow(df_tr)),
             breaks = 10,
             labels = FALSE)
  
# Create the list for each fold 
my.indices <- vector('list',10)
for(i in 1:10){
  my.indices[[i]] <- which(folds != i)
}

cv <- trainControl(method = "cv",
                   index  = my.indices)

# Blueprint
blueprint <- recipe(x     = df,
                    vars  = colnames(df),
                    roles = c(rep('predictor',990),'outcome')) %>% 
  step_zv(all_numeric()) %>%
  step_nzv(all_numeric()) %>%
  step_impute_mean(all_numeric()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric(),threshold=0.9)

blueprint
```

## Linear regression with ridge penalty
```{r}
# Tune Grid
grid <- data.frame(alpha = 0, 
                   lambda = seq(0.01,3,.01))

  
# Train the model
ridge <- caret::train(blueprint, 
                      data      = read_tr, 
                      method    = "glmnet", 
                      trControl = cv,
                      tuneGrid  = grid)
  
ridge$results
ridge$bestTune
  
plot(ridge)
  
predict_te_ridge <- predict(ridge, read_te)

vip(ridge, num_features = 10, geom = "point") + 
  theme_bw()
```

## Linear regression with lasso penalty
```{r}
# Cross validation settings
# Randomly shuffle the data
df_tr <- df_tr[sample(nrow(df_tr)),]

# Create 10 folds with equal size
folds <- cut(seq(1, nrow(df_tr)),
             breaks = 10,
             labels = FALSE)
  
# Create the list for each fold 
my.indices <- vector('list',10)
for(i in 1:10){
  my.indices[[i]] <- which(folds != i)
}

cv <- trainControl(method = "cv",
                   index  = my.indices)
  
# Tune Grid  
# Note that we set the value of alpha to 1 for lasso regression
grid <- data.frame(alpha = 1, lambda = seq(0.01,3,.01)) 
    
# Train the model
lasso <- caret::train(blueprint, 
                      data      = read_tr, 
                      method    = "glmnet", 
                      trControl = cv,
                      tuneGrid  = grid)

lasso$results
  
  
grid <- data.frame(alpha = 1, lambda = seq(0.001,0.015,.001)) 
    
# Train the model
lasso2 <- caret::train(blueprint, 
                      data      = read_tr, 
                      method    = "glmnet", 
                      trControl = cv,
                      tuneGrid  = grid)

lasso2$results
```

## Gradient Boosting Trees
```{r}
# Set the multiple cores for parallel processing
ncores <- 10   

cl <- makePSOCKcluster(ncores)

registerDoParallel(cl)

# Grid Settings  
grid <- expand.grid(shrinkage         = 0.1,
                    n.trees           = 1:1000,
                    interaction.depth = 5,
                    n.minobsinnode    = 10)

gbm1 <- caret::train(blueprint_readability,
                     data         = read_tr,
                     method       = 'gbm',
                     trControl    = cv,
                     tuneGrid     = grid,
                     bag.fraction = 1,
                     verbose      = FALSE)

gbm1$times
```








