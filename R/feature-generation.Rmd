---
title: "Feature Generation"
output: 
    html_document:
      code_download: TRUE
      toc: TRUE
      toc_float:
        collapsed: FALSE
      toc_depth: 3
      code_folding: hide
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      tidy = TRUE, 
                      cache = TRUE, 
                      warning = FALSE, 
                      message = FALSE)
```

```{r}
library(fs)
library(here)
library(rio)

library(tidyverse)
library(magrittr)

library(quanteda)
library(quanteda.textstats)
library(quanteda.dictionaries)
library(udpipe)

theme_set(theme_bw())
```


## Data
```{r}
df <- dir_ls(here("data"), glob = "*.xlsx") %>% 
  map_dfr(., import, .id = "file") %>% 
  mutate(file = str_extract(file, "1a|1b|1c"),
         across(c(file:condition, gender, race), as.factor)
         ) %>% 
  rename(study = file)

finalfit::ff_glimpse(df)

df %>% 
  select(study, p_right:t_right) %>% 
  gather(var, val, -study) %>% 
  ggplot(aes(val)) +
  geom_density() +
  facet_wrap(study ~ var)
```


## Feature generation {.tabset .tabset-fade}
### Summary table
```{r}


```

### Textual statistics with quanteda
```{r}
# tokenize
tokenized <- tokens(df$conflict_text,
                    remove_punct = FALSE,
                    remove_numbers = TRUE,
                    remove_symbols = TRUE,
                    remove_separators = TRUE)

# document-feature matrix
dm <- dfm(tokenized)

# basic statistics and entropy
text_stats <- textstat_summary(dm)

text_stats %<>% 
  mutate(chars = nchar(df$conflict_text),
         sents = nsentence(df$conflict_text),
         words = tokens - puncts,
         entropy = textstat_entropy(dm)$entropy) %>% 
  select(chars:puncts, words, entropy)

df <- bind_cols(df, text_stats)

# word length features
word_lengths <- map(tokenized, nchar)

df %<>% 
  mutate(wl_mean = map(word_lengths, mean),
         wl_median = map(word_lengths, median),
         wl_sd = map(word_lengths, sd),
         wl_min = map(word_lengths, min),
         wl_max = map(word_lengths, max))

# lexical diversity
ld <-
  textstat_lexdiv(dm,
                  remove_numbers = TRUE,
                  remove_punct   = TRUE,
                  remove_symbols = TRUE,
                  measure        = "all")



# measures of readability
mor <-
  textstat_readability(df$conflict_text,
                       measure = "all")

df <- bind_cols(df, mor[,-1])

# features generated
df %>% 
  select(chars:84) %>% 
  names()
```

### Text statistics via udpipe
```{r}
udpipe_download_model(language = "english")

ud_eng <- udpipe_load_model("D:/PhD/2021 Fall/EDLD654 - Machine Learning/text-analysis-of-conflicts/R/english-ewt-ud-2.5-191206.udpipe")

annotated <- udpipe_annotate(ud_eng, x = df$conflict_text)
annotated <- as.data.frame(annotated)
annotated <- cbind_morphological(annotated)

# part of speech tags
pos <-
annotated %>% 
  group_by(doc_id) %>% 
  count(upos) %>% 
  mutate(doc_id = parse_number(doc_id)) %>% 
  arrange(doc_id) %>% 
  pivot_wider(names_from = upos,
              values_from = n)

df <- bind_cols(df, pos[,-1])

# xpos
xpos <-
annotated %>% 
  group_by(doc_id) %>% 
  count(xpos) %>% 
  mutate(doc_id = parse_number(doc_id)) %>% 
  arrange(doc_id) %>% 
  pivot_wider(names_from = xpos,
              values_from = n) %>% 
  rename(comma = 2,
         period = 3,
         quotation = 4)

df <- bind_cols(df, xpos[,-1])

# morphological features
morphs <-
  annotated %>% 
  select(doc_id, starts_with("morph")) 

morph_freq <-
morphs %>% 
  pivot_longer(cols = -doc_id,
               names_to = "var",
               values_to = "val") %>% 
  group_by(doc_id, var) %>% 
  count(val) %>% 
  drop_na() %>% 
  pivot_wider(names_from = c(var, val),
              values_from = n) %>% 
  mutate(doc_id = parse_number(doc_id)) %>% 
  arrange(doc_id)

df <- bind_cols(df, morph_freq[,-1])

# syntactic relations
synts <-
  annotated %>% 
    group_by(doc_id) %>% 
    count(dep_rel) %>% 
    mutate(doc_id = parse_number(doc_id)) %>% 
    arrange(doc_id) %>% 
    pivot_wider(names_from = dep_rel,
                values_from = n)

df <- bind_cols(df, synts[,-1])
```

### Natural Language Proccessing
```{r}



```

### Linguistic inquiry and word count (LIWC)
```{r}
liwc <- import(here("data", "liwc.csv"))


df <- bind_cols(df, liwc)
```

### Moral foundations dictionary
```{r}
mf <- liwcalike(df$conflict_text, 
                dictionary = data_dictionary_MFD) %>% 
  select(care.virtue:sanctity.vice)

df <- bind_cols(df, mf)
```


















